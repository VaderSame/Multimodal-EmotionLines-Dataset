{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from os.path import isfile, isdir, join\n",
    "import yaml, json\n",
    "\n",
    "import librosa \n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from vit_pytorch.efficient import ViT\n",
    "from vit_pytorch import ViT\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, classification_report\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check number of available GPUs\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 142\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# gamma = 0.7\n",
    "# patch_size = 16\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_available_gpus():\n",
    "    if torch.cuda.is_available():\n",
    "        return [(i, torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Let's say you want to use device=1\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./meld.yaml', 'r') as fp:\n",
    "    meld_dict = yaml.safe_load(fp)\n",
    "    \n",
    "train_split = meld_dict['train']\n",
    "test_split = meld_dict['test']\n",
    "dev_split = meld_dict['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aud_key in train_split.keys():\n",
    "    aud_path = f\"./dataset_extracted/output_train_extracted/{aud_key}.wav\"\n",
    "    \n",
    "     # Check if file exists, skip if not\n",
    "    if not os.path.exists(aud_path):\n",
    "        print(f\"File not found, skipping: {aud_path}\")\n",
    "        continue\n",
    "    \n",
    "    aud_properties = train_split[aud_key]\n",
    "    text = aud_properties['Utterance']\n",
    "    emotion_label = aud_properties['Emotion']\n",
    "    # print(aud_path)\n",
    "    # print(text)\n",
    "    # break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aud_key in dev_split.keys():\n",
    "    aud_path = f\"./dataset_extracted/output_dev_extracted/{aud_key}.wav\"\n",
    "    \n",
    "     # Check if file exists, skip if not\n",
    "    if not os.path.exists(aud_path):\n",
    "        print(f\"File not found, skipping: {aud_path}\")\n",
    "        continue\n",
    "    \n",
    "    aud_properties = dev_split[aud_key]\n",
    "    text = aud_properties['Utterance']\n",
    "    emotion_label = aud_properties['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(custom_model, self).__init__()\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.classification = nn.Linear(128*3, 7)\n",
    "    \n",
    "#     def forward(self, x1, x2, x3):\n",
    "#           y1 = model(x1)\n",
    "#           y = torch.cat([y1, y2, y3])\n",
    "#           y = self.activation(y)\n",
    "#           y = self.classification(y)\n",
    "#           y = torch.softmax(y, dim=1)\n",
    "#             return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class meld_dataset(Dataset):\n",
    "    def __init__(self, dataset_path='./meld.yaml', split_name='train', sr=16000, audio_seq_len=10, text_seq_len=512):\n",
    "        super(meld_dataset, self).__init__()\n",
    "        with open(dataset_path, 'r') as fp:\n",
    "            meld_dict = yaml.safe_load(fp)\n",
    "        self.train_split = meld_dict['train']\n",
    "        self.test_split = meld_dict['test']\n",
    "        self.dev_split = meld_dict['dev']\n",
    "        self.split_name = split_name\n",
    "        \n",
    "        self.train_keys = list(self.train_split.keys())\n",
    "        self.test_keys = list(self.test_split.keys())\n",
    "        self.dev_keys = list(self.dev_split.keys())\n",
    "        \n",
    "        self.audio_model = None  # Placeholder for audio model initialization\n",
    "        self.sr = sr\n",
    "        self.audio_seq_len = audio_seq_len\n",
    "        self.text_seq_len = text_seq_len\n",
    "        \n",
    "        self.img_transforms = transforms.Compose([\n",
    "            transforms.Normalize(mean=0.4907024448714564, std=0.3857828166466927),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.split_name == 'train':\n",
    "            return len(self.train_split)\n",
    "        if self.split_name == 'test':\n",
    "            return len(self.test_split)\n",
    "        if self.split_name == 'dev':\n",
    "            return len(self.dev_split)\n",
    "    \n",
    "    # For train split\n",
    "    def __getitem__(self, index):\n",
    "        if self.split_name == 'train':\n",
    "            aud_id = self.train_keys[index]\n",
    "            aud_path = f\"./dataset_extracted/output_train_extracted/{aud_id}.wav\"\n",
    "            audio_input, _ = librosa.load(aud_path)\n",
    "            \n",
    "            # Trimming silence from audio\n",
    "            audio_input, _ = librosa.effects.trim(audio_input, top_db=20)\n",
    "            \n",
    "            spectrogram_db  = librosa.feature.melspectrogram(y=audio_input)\n",
    "            # spectrogram_db = librosa.amplitude_to_db(spectrogram_train, ref=np.max)\n",
    "            text_feature = self.train_split[aud_id]['Utterance']\n",
    "            emotion_label = self.train_split[aud_id]['Emotion']\n",
    "        \n",
    "        # For test split    \n",
    "        elif self.split_name == 'test':\n",
    "            aud_id = self.test_keys[index]\n",
    "            aud_path = f\"./dataset_extracted/output_test_extracted/{aud_id}.wav\"\n",
    "            audio_input, _ = librosa.load(aud_path, sr=self.sr)\n",
    "            \n",
    "            spectrogram  = librosa.feature.melspectrogram(y=audio_input)\n",
    "            spectrogram_db = librosa.amplitude_to_db(spectrogram, ref=np.max)\n",
    "            text_feature = self.test_split[aud_id]['Utterance']\n",
    "            emotion_label = self.test_split[aud_id]['Emotion']\n",
    "        \n",
    "        \n",
    "        # For dev split    \n",
    "        elif self.split_name == 'dev':\n",
    "            aud_id = self.dev_keys[index]\n",
    "            aud_path = f\"./dataset_extracted/output_dev_extracted/{aud_id}.wav\"\n",
    "            audio_input, _ = librosa.load(aud_path, sr=self.sr)\n",
    "            \n",
    "            # Trimming silence from audio\n",
    "            audio_input, _ = librosa.effects.trim(audio_input, top_db=20)\n",
    "        \n",
    "            spectrogram  = librosa.feature.melspectrogram(y=audio_input)\n",
    "            spectrogram_db = librosa.amplitude_to_db(spectrogram, ref=np.max)\n",
    "            text_feature = self.dev_split[aud_id]['Utterance']\n",
    "            emotion_label = self.dev_split[aud_id]['Emotion']\n",
    "        \n",
    "        # print(audio_input.shape[0]/self.sr)\n",
    "        # print(spectrogram_db.shape)\n",
    "        # print(spectrogram_db.shape[1]/(audio_input.shape[0]/self.sr))\n",
    "        \n",
    "        \n",
    "        # Preprocess training data features by trimming/padding text, audio, and spectrogram \n",
    "        # to fixed sequence lengths for consistency during training.\n",
    "        text_feature = text_feature[:min(len(text_feature), self.text_seq_len)] + (\" \"*(self.text_seq_len - len(text_feature)) if len(text_feature) < self.text_seq_len else \"\")\n",
    "        audio_input = audio_input[:min(len(audio_input), self.audio_seq_len * self.sr)]\n",
    "        audio_input = np.pad(audio_input, (0, (self.audio_seq_len * self.sr) - len(audio_input)), 'constant')\n",
    "        spectrogram_db = spectrogram_db[:,:min(spectrogram_db.shape[1], int(self.audio_seq_len * 32))]\n",
    "        spectrogram_db = np.pad(spectrogram_db, ((0, 0), (0, (self.audio_seq_len * 32) - spectrogram_db.shape[1])), 'constant')\n",
    "        \n",
    "        # Convert all training features to tensors\n",
    "        spectrogram_db = spectrogram_db.astype(np.float32)\n",
    "        spectrogram_db = (spectrogram_db - spectrogram_db.min())/(spectrogram_db.max() - spectrogram_db.min())\n",
    "        # print(spectrogram_db.shape)\n",
    "        spectrogram_db = cv2.resize(spectrogram_db, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "        # cv2.imwrite('path_to_image', spectrogram_db)\n",
    "        spectrogram_tensor = torch.from_numpy(spectrogram_db[np.newaxis, ...])*2. -1.\n",
    "        # spectrogram_tensor = self.img_transforms(torch.from_numpy(spectrogram_db[np.newaxis, ...])*2. -1.)\n",
    "        \n",
    "        audio_tensor = torch.from_numpy(audio_input)\n",
    "        text_tensor = torch.from_numpy(np.array([ord(c) for c in text_feature], dtype=np.long))  # Convert text to ASCII values\n",
    "        \n",
    "        \n",
    "        # Convert emotion label to tensor\n",
    "        # You might want to create an emotion_to_idx mapping in __init__\n",
    "        emotion_to_idx = {\n",
    "            'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3,\n",
    "            'joy': 4, 'disgust': 5, 'anger': 6\n",
    "        }\n",
    "        emotion_tensor = torch.tensor(emotion_to_idx[emotion_label], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'spectrogram': spectrogram_tensor,\n",
    "            # 'spectrogram_dev': spectrogram_dev_tensor,\n",
    "            'text_feature': text_tensor,\n",
    "            'audio_feature': audio_tensor,\n",
    "            'emotion_label': emotion_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36196ea4499245d4a66494ef5874a68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1412\n",
      "  warnings.warn(\n",
      "c:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1882\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9970135084738673 0.0019998255346162013\n",
      "4.912767481488786e-07\n"
     ]
    }
   ],
   "source": [
    "dataset = meld_dataset(dataset_path='./meld.yaml', split_name='train')\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "inp_vals = []\n",
    "inp_devs = []\n",
    "for idx, data_dict in enumerate(tqdm(train_loader)):\n",
    "    inps = data_dict['spectrogram']\n",
    "    for b_id in range(inps.shape[0]):\n",
    "        inp_vals.append(torch.sum(inps[b_id], dim=None).item()/(inps.shape[2]*inps.shape[3]))\n",
    "        inp_devs.append(torch.std(inps[b_id], dim=None).item()/(inps.shape[2]*inps.shape[3]))\n",
    "    # if idx == 10:\n",
    "    #     break\n",
    "mean, std_dev = np.mean(np.array(inp_vals)), np.std(np.array(inp_vals))\n",
    "print(mean, std_dev)\n",
    "mean, std_dev = np.mean(np.array(inp_devs)), np.std(np.array(inp_devs))\n",
    "print(mean, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.912767481488786e-07 1.4220140559987843e-07\n"
     ]
    }
   ],
   "source": [
    "mean, std_dev = np.mean(np.array(inp_devs)), np.std(np.array(inp_devs))\n",
    "print(mean, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training device:\n",
    "\n",
    "# device = 'cuda:0'\n",
    "\n",
    "def get_available_gpus():\n",
    "    if torch.cuda.is_available():\n",
    "        return [(i, torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        return []\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define spectrogram dimensions and patch size:\n",
    "spec_height = 256\n",
    "spec_width = 224\n",
    "patch_size = 8\n",
    "\n",
    "# Compute number of patches and sequence length (with class token)\n",
    "num_patches = (spec_height // patch_size) * (spec_width // patch_size)\n",
    "seq_len = num_patches + 1  # +1 for class token\n",
    "\n",
    "# Linear Transformer (adjust seq_len):\n",
    "efficient_transformer = Linformer(dim=128, seq_len=seq_len, depth=12, heads=8, k=64)\n",
    "\n",
    "# Vision Transformer Model for audio spectrogram (using 1 channel):\n",
    "model = ViT(\n",
    "    image_size=spec_height,\n",
    "    patch_size=patch_size,\n",
    "    # num_classes=128,\n",
    "    num_classes=7,\n",
    "    dim = 512,\n",
    "    # dim = 256,\n",
    "    depth = 6,\n",
    "    # depth = 4,\n",
    "    #heads = 8\n",
    "    heads = 4,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    channels=1,\n",
    ").to(device)\n",
    "\n",
    "# Hyperparameters:\n",
    "epochs = 20\n",
    "lr = 1e-4\n",
    "\n",
    "# Loss function, Optimizer and Learning Rate Scheduler:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea34dcfc32504fde817f1c1800a9b60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER 1 loss: 2.2351460456848145\n",
      "ITER 11 loss: 2.4405365857211025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1025\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER 21 loss: 2.44230656396775\n",
      "ITER 31 loss: 2.4245070872768277\n",
      "ITER 41 loss: 2.3960542039173407\n",
      "ITER 51 loss: 2.4161359469095864\n",
      "ITER 61 loss: 2.41752197312527\n",
      "ITER 71 loss: 2.4202051330620136\n",
      "ITER 81 loss: 2.422286804811454\n",
      "ITER 91 loss: 2.418591787526896\n",
      "ITER 101 loss: 2.4161921916621747\n",
      "ITER 111 loss: 2.41721375568493\n",
      "ITER 121 loss: 2.4192012321850487\n",
      "ITER 131 loss: 2.421256507626017\n",
      "ITER 141 loss: 2.4200918674468994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m labels \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\vit_pytorch\\vit.py:114\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m--> 114\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_patch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     b, n, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    117\u001b[0m     cls_tokens \u001b[38;5;241m=\u001b[39m repeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 1 d -> b 1 d\u001b[39m\u001b[38;5;124m'\u001b[39m, b \u001b[38;5;241m=\u001b[39m b)\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\einops\\layers\\torch.py:15\u001b[0m, in \u001b[0;36mRearrange.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m     14\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multirecipe[\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim]\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_for_scriptable_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_axes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\einops\\_torch_specific.py:90\u001b[0m, in \u001b[0;36mapply_for_scriptable_torch\u001b[1;34m(recipe, tensor, reduction_type, axes_dims)\u001b[0m\n\u001b[0;32m     81\u001b[0m (\n\u001b[0;32m     82\u001b[0m     init_shapes,\n\u001b[0;32m     83\u001b[0m     axes_reordering,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     n_axes_w_added,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m=\u001b[39m _reconstruct_from_shape_uncached(recipe, backend\u001b[38;5;241m.\u001b[39mshape(tensor), axes_dims\u001b[38;5;241m=\u001b[39maxes_dims)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init_shapes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axes_reordering \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mtranspose(tensor, axes_reordering)\n",
      "File \u001b[1;32mc:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\einops\\_torch_specific.py:73\u001b[0m, in \u001b[0;36mTorchJitBackend.reshape\u001b[1;34m(x, shape)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(x, shape: List[\u001b[38;5;28mint\u001b[39m]):\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize metrics lists\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "dataset = meld_dataset(dataset_path='./meld.yaml', split_name='train')\n",
    "val_dataset = meld_dataset(dataset_path='./meld.yaml', split_name='dev')\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    iter_loss = 0\n",
    "    print(f\"Epoch: {epoch_num+1}/{epochs}\")\n",
    "    for idx, data_dict in enumerate(tqdm(train_loader)):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get the inputs\n",
    "        input = data_dict['spectrogram'].to(device)\n",
    "        labels = data_dict['emotion_label'].to(device)\n",
    "        # Forward pass\n",
    "        output = model(input)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        # scheduler.step(loss)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = (output.argmax(dim=1) == labels).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "        iter_loss += loss.item()\n",
    "        if idx%10==0:\n",
    "            print(f'ITER {idx+1} loss:', iter_loss/(idx+1))\n",
    "        \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        \n",
    "        for data_dict in val_loader:\n",
    "            inputs = data_dict['spectrogram'].to(device)\n",
    "            labels = data_dict['emotion_label'].to(device)\n",
    "            \n",
    "            val_outputs = model(inputs)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            \n",
    "            acc = (val_outputs.argmax(dim=1) == labels).float().mean()\n",
    "            epoch_val_accuracy += acc / len(val_loader)\n",
    "            epoch_val_loss += val_loss / len(val_loader)\n",
    "        \n",
    "    # Store metrics\n",
    "    train_losses.append(epoch_loss.item())\n",
    "    val_losses.append(epoch_val_loss.item())\n",
    "    train_accuracies.append(epoch_accuracy.item())\n",
    "    val_accuracies.append(epoch_val_accuracy.item())\n",
    "    \n",
    "     # Print metrics\n",
    "    print(\n",
    "        f\"Epoch : {epoch_num + 1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )\n",
    "    # Print current learning rate\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"Learning Rate = {current_lr:.6f}\")\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(epoch_val_loss.item())\n",
    "    \n",
    "# Plot Training and Validation Losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy', marker='o')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy', marker='s')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy vs Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and get a spectrogram\n",
    "dataset = meld_dataset(dataset_path='./meld.yaml', split_name='train')\n",
    "sample_0 = dataset[0]  # Fetch one sample\n",
    "\n",
    "# Extract the spectrogram correctly\n",
    "spectrogram_db_0 = sample_0['spectrogram ']  # Fix key to match dictionary\n",
    "\n",
    "# Plot the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(spectrogram_db_0, sr=16000, x_axis='time', y_axis='mel', cmap='magma')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and get a spectrogram\n",
    "dataset = meld_dataset(dataset_path='./meld.yaml', split_name='train')\n",
    "sample_1 = dataset[1]  # Fetch one sample\n",
    "\n",
    "# Extract the spectrogram correctly\n",
    "spectrogram_db_1 = sample_1['spectrogram ']  # Fix key to match dictionary\n",
    "\n",
    "# Plot the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(spectrogram_db_1, sr=16000, x_axis='time', y_axis='mel', cmap='magma')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sample_0['audio_feature']).plot(figsize=(10, 5),\n",
    "                  lw=1,\n",
    "                  title='Raw Audio Example',\n",
    "                  color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sample_1['audio_feature']).plot(figsize=(10, 5),\n",
    "                  lw=1,\n",
    "                  title='Raw Audio Example',\n",
    "                  color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
