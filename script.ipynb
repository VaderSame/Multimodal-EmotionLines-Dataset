{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from os.path import isfile, isdir, join\n",
    "import yaml, json\n",
    "\n",
    "import librosa \n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from vit_pytorch.efficient import ViT\n",
    "from vit_pytorch import ViT\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, classification_report\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./meld.yaml', 'r') as fp:\n",
    "    meld_dict = yaml.safe_load(fp)\n",
    "    \n",
    "train_split = meld_dict['train']\n",
    "test_split = meld_dict['test']\n",
    "dev_split = meld_dict['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aud_key in train_split.keys():\n",
    "    aud_path = f\"./dataset_extracted/output_train_extracted/{aud_key}.wav\"\n",
    "    \n",
    "     # Check if file exists, skip if not\n",
    "    if not os.path.exists(aud_path):\n",
    "        print(f\"File not found, skipping: {aud_path}\")\n",
    "        continue\n",
    "    \n",
    "    aud_properties = train_split[aud_key]\n",
    "    text = aud_properties['Utterance']\n",
    "    emotion_label = aud_properties['Emotion']\n",
    "    # print(aud_path)\n",
    "    # print(text)\n",
    "    # break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aud_key in dev_split.keys():\n",
    "    aud_path = f\"./dataset_extracted/output_dev_extracted/{aud_key}.wav\"\n",
    "    \n",
    "     # Check if file exists, skip if not\n",
    "    if not os.path.exists(aud_path):\n",
    "        print(f\"File not found, skipping: {aud_path}\")\n",
    "        continue\n",
    "    \n",
    "    aud_properties = dev_split[aud_key]\n",
    "    text = aud_properties['Utterance']\n",
    "    emotion_label = aud_properties['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(custom_model, self).__init__()\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.classification = nn.Linear(128*3, 7)\n",
    "    \n",
    "#     def forward(self, x1, x2, x3):\n",
    "#           y1 = model(x1)\n",
    "#           y = torch.cat([y1, y2, y3])\n",
    "#           y = self.activation(y)\n",
    "#           y = self.classification(y)\n",
    "#           y = torch.softmax(y, dim=1)\n",
    "#             return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class meld_dataset(Dataset):\n",
    "    def __init__(self, dataset_path='./meld.yaml', split_name='train', sr=16000, audio_seq_len=10, text_seq_len=512):\n",
    "        super(meld_dataset, self).__init__()\n",
    "        with open(dataset_path, 'r') as fp:\n",
    "            meld_dict = yaml.safe_load(fp)\n",
    "        self.train_split = meld_dict['train']\n",
    "        self.test_split = meld_dict['test']\n",
    "        self.dev_split = meld_dict['dev']\n",
    "        self.split_name = split_name\n",
    "        \n",
    "        self.train_keys = list(self.train_split.keys())\n",
    "        self.test_keys = list(self.test_split.keys())\n",
    "        self.dev_keys = list(self.dev_split.keys())\n",
    "        \n",
    "        self.audio_model = None  # Placeholder for audio model initialization\n",
    "        self.sr = sr\n",
    "        self.audio_seq_len = audio_seq_len\n",
    "        self.text_seq_len = text_seq_len\n",
    "        \n",
    "        # self.img_transforms = transforms.Compose([\n",
    "        #     transforms.RandomCrop(224),\n",
    "        #     transforms.RandomHorizontalFlip(),\n",
    "        #     transforms.RandomAffine(degrees=45., translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "        # ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.split_name == 'train':\n",
    "            return len(self.train_split)\n",
    "        if self.split_name == 'test':\n",
    "            return len(self.test_split)\n",
    "        if self.split_name == 'dev':\n",
    "            return len(self.dev_split)\n",
    "    \n",
    "    # For train split\n",
    "    def __getitem__(self, index):\n",
    "        if self.split_name == 'train':\n",
    "            aud_id = self.train_keys[index]\n",
    "            aud_path = f\"./dataset_extracted/output_train_extracted/{aud_id}.wav\"\n",
    "            audio_input, _ = librosa.load(aud_path, sr=self.sr)\n",
    "            \n",
    "            # Trimming silence from audio\n",
    "            audio_input, _ = librosa.effects.trim(audio_input, top_db=20)\n",
    "            \n",
    "            spectrogram  = librosa.feature.melspectrogram(y=audio_input)\n",
    "            spectrogram_db = librosa.amplitude_to_db(spectrogram, ref=np.max)\n",
    "            text_feature = self.train_split[aud_id]['Utterance']\n",
    "            emotion_label = self.train_split[aud_id]['Emotion']\n",
    "        \n",
    "        # For test split    \n",
    "        elif self.split_name == 'test':\n",
    "            aud_id = self.test_keys[index]\n",
    "            aud_path = f\"./dataset_extracted/output_test_extracted/{aud_id}.wav\"\n",
    "            audio_input, _ = librosa.load(aud_path, sr=self.sr)\n",
    "            \n",
    "            spectrogram  = librosa.feature.melspectrogram(y=audio_input)\n",
    "            text_feature = self.test_split[aud_id]['Utterance']\n",
    "            emotion_label = self.test_split[aud_id]['Emotion']\n",
    "        \n",
    "        \n",
    "        # For dev split    \n",
    "        elif self.split_name == 'dev':\n",
    "            aud_dev_id = self.dev_keys[index]\n",
    "            aud_dev_path = f\"./dataset_extracted/output_dev_extracted/{aud_dev_id}.wav\"\n",
    "            audio_dev_input, _ = librosa.load(aud_dev_path, sr=self.sr)\n",
    "            \n",
    "            # Trimming silence from audio\n",
    "            audio_dev_input, _ = librosa.effects.trim(audio_dev_input, top_db=20)\n",
    "        \n",
    "            spectrogram_dev  = librosa.feature.melspectrogram(y=audio_dev_input)\n",
    "            spectrogram_dev_db = librosa.amplitude_to_db(spectrogram_dev, ref=np.max)\n",
    "            \n",
    "            text_feature = self.dev_split[aud_id]['Utterance']\n",
    "            emotion_label = self.dev_split[aud_id]['Emotion']\n",
    "        \n",
    "        # print(audio_input.shape[0]/self.sr)\n",
    "        # print(spectrogram_db.shape)\n",
    "        # print(spectrogram_db.shape[1]/(audio_input.shape[0]/self.sr))\n",
    "        \n",
    "        \n",
    "        # Preprocess training data features by trimming/padding text, audio, and spectrogram \n",
    "        # to fixed sequence lengths for consistency during training.\n",
    "        text_feature = text_feature[:min(len(text_feature), self.text_seq_len)] + (\" \"*(self.text_seq_len - len(text_feature)) if len(text_feature) < self.text_seq_len else \"\")\n",
    "        audio_input = audio_input[:min(len(audio_input), self.audio_seq_len * self.sr)]\n",
    "        audio_input = np.pad(audio_input, (0, (self.audio_seq_len * self.sr) - len(audio_input)), 'constant')\n",
    "        spectrogram_db = spectrogram_db[:,:min(spectrogram_db.shape[1], int(self.audio_seq_len * 32))]\n",
    "        spectrogram_db = np.pad(spectrogram_db, ((0, 0), (0, (self.audio_seq_len * 32) - spectrogram_db.shape[1])), 'constant')\n",
    "        \n",
    "        # Convert all training features to tensors\n",
    "        spectrogram_db = spectrogram_db.astype(np.float32)\n",
    "        spectrogram_db = (spectrogram_db - spectrogram_db.min())/(spectrogram_db.max() - spectrogram_db.min())\n",
    "        spectrogram_db = cv2.resize(spectrogram_db, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "        spectrogram_tensor = torch.from_numpy(spectrogram_db[np.newaxis, ...])\n",
    "        \n",
    "        \n",
    "        \n",
    "        audio_tensor = torch.from_numpy(audio_input)\n",
    "        text_tensor = torch.from_numpy(np.array([ord(c) for c in text_feature], dtype=np.long))  # Convert text to ASCII values\n",
    "        \n",
    "        # Convert emotion label to tensor\n",
    "        # You might want to create an emotion_to_idx mapping in __init__\n",
    "        emotion_to_idx = {\n",
    "            'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3,\n",
    "            'joy': 4, 'disgust': 5, 'anger': 6\n",
    "        }\n",
    "        emotion_tensor = torch.tensor(emotion_to_idx[emotion_label], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'spectrogram': spectrogram_tensor,\n",
    "            'text_feature': text_tensor,\n",
    "            'audio_feature': audio_tensor,\n",
    "            'emotion_label': emotion_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 142\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hyperparameters:\n",
    "epochs = 15\n",
    "lr = 0.01\n",
    "\n",
    "# gamma = 0.7\n",
    "# patch_size = 16\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training device:\n",
    "\n",
    "# device = 'cuda:0'\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define spectrogram dimensions and patch size:\n",
    "spec_height = 256\n",
    "spec_width = 224\n",
    "patch_size = 16\n",
    "\n",
    "# Compute number of patches and sequence length (with class token)\n",
    "num_patches = (spec_height // patch_size) * (spec_width // patch_size)\n",
    "seq_len = num_patches + 1  # +1 for class token\n",
    "\n",
    "# Linear Transformer (adjust seq_len):\n",
    "efficient_transformer = Linformer(dim=128, seq_len=seq_len, depth=12, heads=8, k=64)\n",
    "\n",
    "# Vision Transformer Model for audio spectrogram (using 1 channel):\n",
    "model = ViT(\n",
    "    image_size=spec_height,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=128,\n",
    "    # transformer=efficient_transformer,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    channels=1,\n",
    ").to(device)\n",
    "model.train()\n",
    "\n",
    "# Loss function, Optimizer and Learning Rate Scheduler:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "dataset = meld_dataset(dataset_path='./meld.yaml', split_name='train')\n",
    "dataloader_obj = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "    print(f\"Epoch: {epoch_num+1}/{epochs}\")\n",
    "    for idx, data_dict in enumerate(tqdm(dataloader_obj)):\n",
    "        # print(data_dict['spectrogram'].shape)\n",
    "        # print(data_dict['text_feature'].shape)\n",
    "        # print(data_dict['audio_feature'].shape)\n",
    "        # print(data_dict['emotion_label'].shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input = data_dict['spectrogram'].to(device)\n",
    "        labels = data_dict['emotion_label'].to(device)\n",
    "        output = model(input)\n",
    "        output  = torch.softmax(output, dim=1)\n",
    "        # output  = torch.argmax(output, dim=1)\n",
    "        # print(output.shape)\n",
    "        # print(output)\n",
    "        # print(labels)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        scheduler.step(loss)\n",
    "    #     break\n",
    "    # break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and get a spectrogram\n",
    "dataset = meld_dataset(dataset_path='./meld.yaml', split_name='train')\n",
    "sample_0 = dataset[0]  # Fetch one sample\n",
    "\n",
    "# Extract the spectrogram correctly\n",
    "spectrogram_db_0 = sample_0['spectrogram ']  # Fix key to match dictionary\n",
    "\n",
    "# Plot the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(spectrogram_db_0, sr=16000, x_axis='time', y_axis='mel', cmap='magma')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and get a spectrogram\n",
    "dataset = meld_dataset(dataset_path='./meld.yaml', split_name='train')\n",
    "sample_1 = dataset[1]  # Fetch one sample\n",
    "\n",
    "# Extract the spectrogram correctly\n",
    "spectrogram_db_1 = sample_1['spectrogram ']  # Fix key to match dictionary\n",
    "\n",
    "# Plot the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(spectrogram_db_1, sr=16000, x_axis='time', y_axis='mel', cmap='magma')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sample_0['audio_feature']).plot(figsize=(10, 5),\n",
    "                  lw=1,\n",
    "                  title='Raw Audio Example',\n",
    "                  color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sample_1['audio_feature']).plot(figsize=(10, 5),\n",
    "                  lw=1,\n",
    "                  title='Raw Audio Example',\n",
    "                  color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
