{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b541a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml , os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5976e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "c:\\UWO\\Projects\\Multimodal Emotion detection\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Whisper\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2abf6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./meld.yaml', 'r') as fp:\n",
    "    meld_dict = yaml.safe_load(fp)\n",
    "    \n",
    "train_split = meld_dict['train']\n",
    "test_split = meld_dict['test']\n",
    "dev_split = meld_dict['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92b5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aud_key in train_split.keys():\n",
    "    aud_path = os.path.normpath(os.path.join(\"./dataset_extracted/output_train_extracted\", f\"{aud_key}.wav\"))\n",
    "    \n",
    "    if not os.path.exists(aud_path):\n",
    "        print(f\"File not found, skipping: {aud_path}\")\n",
    "        continue\n",
    "\n",
    "    aud_properties = train_split[aud_key]\n",
    "    text = aud_properties['Utterance']\n",
    "    emotion_label = aud_properties['Emotion']\n",
    "    \n",
    "\n",
    "for aud_key in dev_split.keys():\n",
    "    aud_path = os.path.normpath(os.path.join(\"./dataset_extracted/output_dev_extracted\", f\"{aud_key}.wav\"))\n",
    "\n",
    "    if not os.path.exists(aud_path):\n",
    "        print(f\"File not found, skipping: {aud_path}\")\n",
    "        continue\n",
    "\n",
    "    aud_properties = dev_split[aud_key]\n",
    "    text = aud_properties['Utterance']\n",
    "    emotion_label = aud_properties['Emotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6715ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract mean pooled encoder features from audio\n",
    "# import whisperx\n",
    "# whisper_model = whisperx.load_model(\"base\")\n",
    "# whisper_model.eval()\n",
    "# whisper_model = whisper_model.to(device)\n",
    "\n",
    "# def extract_whisper_features(audio_path):\n",
    "#     # audio = whisper.load_audio(audio_path)\n",
    "#     from pathlib import Path\n",
    "#     audio_path = Path(audio_path).as_posix()\n",
    "#     audio = whisperx.load_audio(audio_path)\n",
    "\n",
    "#     audio = whisperx.pad_or_trim(audio)\n",
    "#     mel = whisperx.log_mel_spectrogram(audio).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         features = whisper_model.encoder(mel.unsqueeze(0))  # shape: [1, frames, dim]\n",
    "#     return features.mean(dim=1).squeeze().cpu().numpy()  # shape: [dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c8b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "import torch\n",
    "from whisper.model import AudioEncoder\n",
    "from whisper import _MODELS, _ALIGNMENT_HEADS, _download, available_models\n",
    "from whisper import ModelDimensions\n",
    "from typing import Optional, Union\n",
    "\n",
    "def load_model(\n",
    "    model,\n",
    "    name: str,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    download_root: str = None,\n",
    "    in_memory: bool = False,\n",
    ") -> AudioEncoder:\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if download_root is None:\n",
    "        default = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n",
    "        download_root = os.path.join(os.getenv(\"XDG_CACHE_HOME\", default), \"whisper\")\n",
    "\n",
    "    if name in _MODELS:\n",
    "        checkpoint_file = _download(_MODELS[name], download_root, in_memory)\n",
    "        alignment_heads = _ALIGNMENT_HEADS[name]\n",
    "    elif os.path.isfile(name):\n",
    "        checkpoint_file = open(name, \"rb\").read() if in_memory else name\n",
    "        alignment_heads = None\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Model {name} not found; available models = {available_models()}\"\n",
    "        )\n",
    "\n",
    "    with (\n",
    "        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, \"rb\")\n",
    "    ) as fp:\n",
    "        checkpoint = torch.load(fp, map_location=device)\n",
    "    del checkpoint_file\n",
    "\n",
    "    dims = ModelDimensions(**checkpoint[\"dims\"])\n",
    "    print(\n",
    "        dims.n_mels,\n",
    "        dims.n_audio_ctx,\n",
    "        dims.n_audio_state,\n",
    "        dims.n_audio_head,\n",
    "        dims.n_audio_layer,\n",
    "    )\n",
    "    # model = AudioEncoder(\n",
    "    #     dims.n_mels,\n",
    "    #     dims.n_audio_ctx,\n",
    "    #     dims.n_audio_state,\n",
    "    #     dims.n_audio_head,\n",
    "    #     dims.n_audio_layer,\n",
    "    # )\n",
    "\n",
    "    model_state_dict = model.state_dict()\n",
    "    print(\"\\n\".join([f for f in checkpoint[\"model_state_dict\"].keys() if \"encoder\" in f and f in model_state_dict.keys()]))\n",
    "    encoder_keys = [f for f in checkpoint[\"model_state_dict\"].keys() if \"encoder\" in f]\n",
    "    missing_keys, unexpected_keys = model.load_state_dict({f: checkpoint[\"model_state_dict\"][f] for f in encoder_keys}, \n",
    "                          strict=False)\n",
    "    # if alignment_heads is not None:\n",
    "    #     model.set_alignment_heads(alignment_heads)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8343dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Literal\n",
    "\n",
    "import librosa\n",
    "import whisper\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# whisper_model = whisper.load_model(\"base\")\n",
    "# whisper_model.eval()\n",
    "# whisper_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "class WhisperMELDDataset(Dataset):\n",
    "    def __init__(self, dataset_path='./meld.yaml', split_name='train', sr=16000, label_encoder=None, \n",
    "                 mode: Literal[\"default\", \"temporal\", \"full\"]=\"default\", whisper_model=None, max_len=128):\n",
    "        super(WhisperMELDDataset, self).__init__()\n",
    "\n",
    "        with open(dataset_path, 'r') as fp:\n",
    "            meld_dict = yaml.safe_load(fp)\n",
    "        \n",
    "        self.split_name = split_name\n",
    "        self.sr = sr\n",
    "        self.mode = mode\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Extract correct split\n",
    "        if split_name == 'train':\n",
    "            self.split = meld_dict['train']\n",
    "            self.audio_dir = \"./dataset_extracted/output_train_extracted\"\n",
    "        elif split_name == 'test':\n",
    "            self.split = meld_dict['test']\n",
    "            self.audio_dir = \"./dataset_extracted/output_test_extracted\"\n",
    "        elif split_name == 'dev':\n",
    "            self.split = meld_dict['dev']\n",
    "            self.audio_dir = \"./dataset_extracted/output_dev_extracted\"\n",
    "        else:\n",
    "            raise ValueError(\"split_name must be one of: train, test, dev\")\n",
    "\n",
    "        self.whisper_model = whisper_model\n",
    "        self.keys = list(self.split.keys())\n",
    "\n",
    "        # Build label encoder if not provided\n",
    "        if label_encoder is None:\n",
    "            all_labels = [entry['Emotion'] for entry in self.split.values()]\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.label_encoder.fit(all_labels)\n",
    "        else:\n",
    "            self.label_encoder = label_encoder\n",
    "\n",
    "        # Pre-encode all labels\n",
    "        self.encoded_labels = {\n",
    "            k: self.label_encoder.transform([v['Emotion']])[0]\n",
    "            for k, v in self.split.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def extract_whisper_features(self, audio_path):\n",
    "        # audio = whisper.load_audio(audio_path)\n",
    "        # audio_path = Path(audio_path).as_posix()\n",
    "        # print(audio_path, isfile(audio_path))\n",
    "        try:\n",
    "            audio, _ = librosa.load(audio_path, sr=self.sr)\n",
    "            # audio = whisper.load_audio(join(os.getcwd(), audio_path))\n",
    "            audio = whisper.pad_or_trim(audio)    \n",
    "            mel = whisper.log_mel_spectrogram(audio).to(device)\n",
    "            \n",
    "            if self.mode!=\"full\":\n",
    "                with torch.no_grad():\n",
    "                    features = self.whisper_model.encoder(mel.unsqueeze(0))  # [1, T, 768]\n",
    "                    # print(features.shape)\n",
    "            else:\n",
    "                feat = torch.zeros(1024).to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to load audio {audio_path}: {e} | Exists:{isfile(audio_path)}\")\n",
    "            feat = torch.zeros(1024).to(device)\n",
    "            \n",
    "        if self.mode==\"default\":\n",
    "            mn = features.mean(dim=1).squeeze()\n",
    "            std = features.std(dim=1).squeeze()\n",
    "            feat = torch.concatenate([mn, std], dim=0)\n",
    "            return feat\n",
    "        elif self.mode==\"temporal\":\n",
    "            feat = features.detach()\n",
    "            return feat\n",
    "        elif self.mode==\"full\":\n",
    "            return mel\n",
    "        return feat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aud_key = self.keys[idx]\n",
    "        # audio_path = os.path.join(self.audio_dir, f\"{aud_key}.wav\")\n",
    "        # audio_path = os.path.normpath(os.path.join(self.audio_dir, f\"{aud_key}.wav\"))\n",
    "        # audio_path = Path(audio_path).as_posix()\n",
    "        audio_path = os.path.join(self.audio_dir, f\"{aud_key}.wav\")\n",
    "        features = self.extract_whisper_features(audio_path)\n",
    "        \n",
    "        # Get text for corresponding utterance\n",
    "        utterance = self.split[aud_key]['Utterance']\n",
    "        \n",
    "        # Tokenize text (BERT input)\n",
    "        encoding = self.tokenizer(\n",
    "        utterance,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=self.max_len,\n",
    "        return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze(0)  # (max_len,)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)  # (max_len,)\n",
    "        label = self.encoded_labels[aud_key]\n",
    "        return features, input_ids, attention_mask, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare label encoder manually using meld.yaml (for reproducibility)\n",
    "import yaml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "with open('C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', 'r') as fp:\n",
    "    meld_dict = yaml.safe_load(fp)\n",
    "\n",
    "# Use the train split to fit the encoder\n",
    "all_labels = [entry['Emotion'] for entry in meld_dict['train'].values()]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Step 2: Instantiate datasets\n",
    "# train_dataset = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='train', label_encoder=label_encoder)\n",
    "# dev_dataset   = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='dev',   label_encoder=label_encoder)\n",
    "# test_dataset  = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='test',  label_encoder=label_encoder)\n",
    "# train_dataset = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='train', label_encoder=label_encoder, mode=\"temporal\")\n",
    "# dev_dataset   = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='dev',   label_encoder=label_encoder, mode=\"temporal\")\n",
    "# test_dataset  = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='test',  label_encoder=label_encoder, mode=\"temporal\")\n",
    "train_dataset = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='train', label_encoder=label_encoder, mode=\"full\")\n",
    "dev_dataset   = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='dev',   label_encoder=label_encoder, mode=\"full\")\n",
    "test_dataset  = WhisperMELDDataset(dataset_path='C:/UWO/Projects/Multimodal Emotion detection/meld.yaml', split_name='test',  label_encoder=label_encoder, mode=\"full\")\n",
    "\n",
    "# Step 3: Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_dataset, batch_size=8)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d21ee08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848867bba39145dc946829bf67fec404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found batch with features shape: torch.Size([8, 80, 3000]), labels shape: torch.Size([8])\n",
      "torch.Size([9988])\n"
     ]
    }
   ],
   "source": [
    "label_arr = []\n",
    "for i, (features, input_ids, attention_mask, label) in enumerate(tqdm(train_loader)):\n",
    "    label_arr.append(label.cpu())\n",
    "    if i == 0:  # Just print the first batch\n",
    "        print(f\"Found batch with features shape: {features.shape}, labels shape: {label.shape}\")\n",
    "    # break\n",
    "label_arr = torch.concatenate(label_arr, dim=0).to(device)\n",
    "print(label_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c308b411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4476b054c1471084b3d885ee477fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found batch with features shape: torch.Size([8, 80, 3000]), labels shape: torch.Size([8]), input_shape: torch.Size([8, 128]) | torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "for i, (features, input_ids, attention_mask, label) in enumerate(tqdm(train_loader)):\n",
    "    if i == 0:  # Just print the first batch\n",
    "        print(f\"Found batch with features shape: {features.shape}, labels shape: {label.shape}, input_shape: {input_ids.shape} | {attention_mask.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a817ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample paths from training set:\n",
      "Train path 1: ./dataset_extracted/output_train_extracted\\dia0_utt0.wav\n",
      "File exists: True\n",
      "Train path 2: ./dataset_extracted/output_train_extracted\\dia0_utt1.wav\n",
      "File exists: True\n",
      "Train path 3: ./dataset_extracted/output_train_extracted\\dia0_utt10.wav\n",
      "File exists: True\n",
      "Train path 4: ./dataset_extracted/output_train_extracted\\dia0_utt11.wav\n",
      "File exists: True\n",
      "Train path 5: ./dataset_extracted/output_train_extracted\\dia0_utt12.wav\n",
      "File exists: True\n",
      "\n",
      "Sample paths from validation set:\n",
      "Dev path 1: ./dataset_extracted/output_dev_extracted\\dia0_utt0.wav\n",
      "File exists: True\n",
      "Dev path 2: ./dataset_extracted/output_dev_extracted\\dia0_utt1.wav\n",
      "File exists: True\n",
      "Dev path 3: ./dataset_extracted/output_dev_extracted\\dia100_utt0.wav\n",
      "File exists: True\n",
      "Dev path 4: ./dataset_extracted/output_dev_extracted\\dia101_utt0.wav\n",
      "File exists: True\n",
      "Dev path 5: ./dataset_extracted/output_dev_extracted\\dia102_utt0.wav\n",
      "File exists: True\n",
      "\n",
      "Sample paths from test set:\n",
      "Test path 1: ./dataset_extracted/output_test_extracted\\dia0_utt0.wav\n",
      "File exists: True\n",
      "Test path 2: ./dataset_extracted/output_test_extracted\\dia0_utt1.wav\n",
      "File exists: True\n",
      "Test path 3: ./dataset_extracted/output_test_extracted\\dia0_utt2.wav\n",
      "File exists: True\n",
      "Test path 4: ./dataset_extracted/output_test_extracted\\dia100_utt0.wav\n",
      "File exists: True\n",
      "Test path 5: ./dataset_extracted/output_test_extracted\\dia100_utt1.wav\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample paths from training set:\")\n",
    "for i, key in enumerate(train_dataset.keys[:5]):  # Print first 5 paths\n",
    "    audio_path = os.path.join(train_dataset.audio_dir, f\"{key}.wav\")\n",
    "    print(f\"Train path {i+1}: {audio_path}\")\n",
    "    print(f\"File exists: {os.path.exists(audio_path)}\")\n",
    "\n",
    "print(\"\\nSample paths from validation set:\")\n",
    "for i, key in enumerate(dev_dataset.keys[:5]):  # Print first 5 paths\n",
    "    audio_path = os.path.join(dev_dataset.audio_dir, f\"{key}.wav\")\n",
    "    print(f\"Dev path {i+1}: {audio_path}\")\n",
    "    print(f\"File exists: {os.path.exists(audio_path)}\")\n",
    "\n",
    "print(\"\\nSample paths from test set:\")\n",
    "for i, key in enumerate(test_dataset.keys[:5]):  # Print first 5 paths\n",
    "    audio_path = os.path.join(test_dataset.audio_dir, f\"{key}.wav\")\n",
    "    print(f\"Test path {i+1}: {audio_path}\")\n",
    "    print(f\"File exists: {os.path.exists(audio_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6707ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.286616  5.2651553 5.324094  0.8186214 0.3030064 2.0891027 1.1841139]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# label_arr = (label_arr.shape[0] - torch.sum(label_arr, dim=1))/label_arr.shape[0]\n",
    "label_weights = compute_class_weight(class_weight=\"balanced\", classes=np.arange(7), y=label_arr.cpu().numpy())\n",
    "label_weights = label_weights.astype(np.float32)\n",
    "print(label_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa957cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semal\\AppData\\Local\\Temp\\ipykernel_33624\\3321746527.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 1500 512 8 6\n",
      "encoder.positional_embedding\n",
      "encoder.conv1.weight\n",
      "encoder.conv1.bias\n",
      "encoder.conv2.weight\n",
      "encoder.conv2.bias\n",
      "encoder.blocks.0.mlp_ln.weight\n",
      "encoder.blocks.0.mlp_ln.bias\n",
      "encoder.blocks.0.mlp.0.weight\n",
      "encoder.blocks.0.mlp.0.bias\n",
      "encoder.blocks.0.mlp.2.weight\n",
      "encoder.blocks.0.mlp.2.bias\n",
      "encoder.blocks.0.attn_ln.weight\n",
      "encoder.blocks.0.attn_ln.bias\n",
      "encoder.blocks.0.attn.query.weight\n",
      "encoder.blocks.0.attn.query.bias\n",
      "encoder.blocks.0.attn.key.weight\n",
      "encoder.blocks.0.attn.value.weight\n",
      "encoder.blocks.0.attn.value.bias\n",
      "encoder.blocks.0.attn.out.weight\n",
      "encoder.blocks.0.attn.out.bias\n",
      "encoder.blocks.1.mlp_ln.weight\n",
      "encoder.blocks.1.mlp_ln.bias\n",
      "encoder.blocks.1.mlp.0.weight\n",
      "encoder.blocks.1.mlp.0.bias\n",
      "encoder.blocks.1.mlp.2.weight\n",
      "encoder.blocks.1.mlp.2.bias\n",
      "encoder.blocks.1.attn_ln.weight\n",
      "encoder.blocks.1.attn_ln.bias\n",
      "encoder.blocks.1.attn.query.weight\n",
      "encoder.blocks.1.attn.query.bias\n",
      "encoder.blocks.1.attn.key.weight\n",
      "encoder.blocks.1.attn.value.weight\n",
      "encoder.blocks.1.attn.value.bias\n",
      "encoder.blocks.1.attn.out.weight\n",
      "encoder.blocks.1.attn.out.bias\n",
      "encoder.blocks.2.mlp_ln.weight\n",
      "encoder.blocks.2.mlp_ln.bias\n",
      "encoder.blocks.2.mlp.0.weight\n",
      "encoder.blocks.2.mlp.0.bias\n",
      "encoder.blocks.2.mlp.2.weight\n",
      "encoder.blocks.2.mlp.2.bias\n",
      "encoder.blocks.2.attn_ln.weight\n",
      "encoder.blocks.2.attn_ln.bias\n",
      "encoder.blocks.2.attn.query.weight\n",
      "encoder.blocks.2.attn.query.bias\n",
      "encoder.blocks.2.attn.key.weight\n",
      "encoder.blocks.2.attn.value.weight\n",
      "encoder.blocks.2.attn.value.bias\n",
      "encoder.blocks.2.attn.out.weight\n",
      "encoder.blocks.2.attn.out.bias\n",
      "encoder.blocks.3.mlp_ln.weight\n",
      "encoder.blocks.3.mlp_ln.bias\n",
      "encoder.blocks.3.mlp.0.weight\n",
      "encoder.blocks.3.mlp.0.bias\n",
      "encoder.blocks.3.mlp.2.weight\n",
      "encoder.blocks.3.mlp.2.bias\n",
      "encoder.blocks.3.attn_ln.weight\n",
      "encoder.blocks.3.attn_ln.bias\n",
      "encoder.blocks.3.attn.query.weight\n",
      "encoder.blocks.3.attn.query.bias\n",
      "encoder.blocks.3.attn.key.weight\n",
      "encoder.blocks.3.attn.value.weight\n",
      "encoder.blocks.3.attn.value.bias\n",
      "encoder.blocks.3.attn.out.weight\n",
      "encoder.blocks.3.attn.out.bias\n",
      "encoder.blocks.4.mlp_ln.weight\n",
      "encoder.blocks.4.mlp_ln.bias\n",
      "encoder.blocks.4.mlp.0.weight\n",
      "encoder.blocks.4.mlp.0.bias\n",
      "encoder.blocks.4.mlp.2.weight\n",
      "encoder.blocks.4.mlp.2.bias\n",
      "encoder.blocks.4.attn_ln.weight\n",
      "encoder.blocks.4.attn_ln.bias\n",
      "encoder.blocks.4.attn.query.weight\n",
      "encoder.blocks.4.attn.query.bias\n",
      "encoder.blocks.4.attn.key.weight\n",
      "encoder.blocks.4.attn.value.weight\n",
      "encoder.blocks.4.attn.value.bias\n",
      "encoder.blocks.4.attn.out.weight\n",
      "encoder.blocks.4.attn.out.bias\n",
      "encoder.blocks.5.mlp_ln.weight\n",
      "encoder.blocks.5.mlp_ln.bias\n",
      "encoder.blocks.5.mlp.0.weight\n",
      "encoder.blocks.5.mlp.0.bias\n",
      "encoder.blocks.5.mlp.2.weight\n",
      "encoder.blocks.5.mlp.2.bias\n",
      "encoder.blocks.5.attn_ln.weight\n",
      "encoder.blocks.5.attn_ln.bias\n",
      "encoder.blocks.5.attn.query.weight\n",
      "encoder.blocks.5.attn.query.bias\n",
      "encoder.blocks.5.attn.key.weight\n",
      "encoder.blocks.5.attn.value.weight\n",
      "encoder.blocks.5.attn.value.bias\n",
      "encoder.blocks.5.attn.out.weight\n",
      "encoder.blocks.5.attn.out.bias\n",
      "encoder.ln_post.weight\n",
      "encoder.ln_post.bias\n",
      "WhisperClassifier(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (blocks): ModuleList(\n",
      "      (0-5): 6 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (conv1): Conv1d(1500, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (bn_conv1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_conv1): ELU(alpha=1.0)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (compressor): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=1792, out_features=1024, bias=True)\n",
      "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ELU(alpha=1.0)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ELU(alpha=1.0)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (relu3): ELU(alpha=1.0)\n",
      "  (dropout3): Dropout(p=0.2, inplace=False)\n",
      "  (classifier): Linear(in_features=64, out_features=7, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f774a3c753420d8ef0350c5c2fa231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 1.8508 | [1e-05]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d32f0ceb3de4766808aaa614dd4f5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.45      0.39      0.41       153\n",
      "     disgust       0.05      0.09      0.06        22\n",
      "        fear       0.03      0.03      0.03        40\n",
      "         joy       0.52      0.48      0.50       163\n",
      "     neutral       0.81      0.46      0.59       469\n",
      "     sadness       0.36      0.42      0.39       111\n",
      "    surprise       0.36      0.83      0.51       150\n",
      "\n",
      "    accuracy                           0.48      1108\n",
      "   macro avg       0.37      0.39      0.35      1108\n",
      "weighted avg       0.57      0.48      0.49      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd47b4f6dd1249b68236c9a65205f99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training Loss: 1.5457 | [8.888888888888888e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3120c6e68a43648e72ef99f0a59ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.48      0.37      0.41       153\n",
      "     disgust       0.26      0.23      0.24        22\n",
      "        fear       0.26      0.12      0.17        40\n",
      "         joy       0.51      0.62      0.56       163\n",
      "     neutral       0.77      0.73      0.75       469\n",
      "     sadness       0.40      0.45      0.42       111\n",
      "    surprise       0.54      0.67      0.60       150\n",
      "\n",
      "    accuracy                           0.59      1108\n",
      "   macro avg       0.46      0.46      0.45      1108\n",
      "weighted avg       0.60      0.59      0.59      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35feb56c4f2346b4ba19b264c1175e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training Loss: 1.3808 | [7.77777777777778e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f074448813564ed3a68ff7b662d95a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.50      0.33      0.40       153\n",
      "     disgust       0.15      0.23      0.18        22\n",
      "        fear       0.14      0.15      0.14        40\n",
      "         joy       0.56      0.58      0.57       163\n",
      "     neutral       0.78      0.67      0.72       469\n",
      "     sadness       0.36      0.50      0.42       111\n",
      "    surprise       0.52      0.72      0.60       150\n",
      "\n",
      "    accuracy                           0.57      1108\n",
      "   macro avg       0.43      0.45      0.43      1108\n",
      "weighted avg       0.60      0.57      0.58      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cf6ff4b57e4a948d5ce51641bb36ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training Loss: 1.1874 | [6.666666666666667e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecda8fd91dc4531a1acc7ed76e1b737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.54      0.40      0.46       153\n",
      "     disgust       0.29      0.36      0.32        22\n",
      "        fear       0.12      0.28      0.17        40\n",
      "         joy       0.52      0.60      0.55       163\n",
      "     neutral       0.77      0.68      0.72       469\n",
      "     sadness       0.45      0.33      0.38       111\n",
      "    surprise       0.53      0.69      0.60       150\n",
      "\n",
      "    accuracy                           0.57      1108\n",
      "   macro avg       0.46      0.48      0.46      1108\n",
      "weighted avg       0.60      0.57      0.58      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3e03b1bb754005be995ec3b8e24203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training Loss: 1.0031 | [5.555555555555557e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cbf3f891034091a1f1fe1fc47cfbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.51      0.42      0.46       153\n",
      "     disgust       0.19      0.41      0.26        22\n",
      "        fear       0.26      0.28      0.27        40\n",
      "         joy       0.47      0.61      0.53       163\n",
      "     neutral       0.78      0.69      0.73       469\n",
      "     sadness       0.48      0.36      0.41       111\n",
      "    surprise       0.54      0.65      0.59       150\n",
      "\n",
      "    accuracy                           0.58      1108\n",
      "   macro avg       0.46      0.49      0.47      1108\n",
      "weighted avg       0.61      0.58      0.59      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06132aa272045d887124319d4e0dd61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training Loss: 0.7894 | [4.444444444444444e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794de45eeb5a4fe293a0e79af263e1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.53      0.47      0.50       153\n",
      "     disgust       0.27      0.27      0.27        22\n",
      "        fear       0.24      0.25      0.24        40\n",
      "         joy       0.49      0.63      0.55       163\n",
      "     neutral       0.79      0.68      0.73       469\n",
      "     sadness       0.48      0.39      0.43       111\n",
      "    surprise       0.50      0.69      0.58       150\n",
      "\n",
      "    accuracy                           0.59      1108\n",
      "   macro avg       0.47      0.48      0.47      1108\n",
      "weighted avg       0.61      0.59      0.59      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5084c734bccd47f2ab86caa044dabc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training Loss: 0.6329 | [3.3333333333333333e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a367df3af6f4c3c9009d34d2eff7a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.46      0.54      0.49       153\n",
      "     disgust       0.26      0.41      0.32        22\n",
      "        fear       0.19      0.20      0.20        40\n",
      "         joy       0.64      0.48      0.55       163\n",
      "     neutral       0.76      0.74      0.75       469\n",
      "     sadness       0.49      0.43      0.46       111\n",
      "    surprise       0.53      0.61      0.56       150\n",
      "\n",
      "    accuracy                           0.60      1108\n",
      "   macro avg       0.47      0.49      0.47      1108\n",
      "weighted avg       0.61      0.60      0.60      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c77cc1a9e4b4067a66995ff5d3a7670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training Loss: 0.5157 | [2.222222222222222e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0686cf279fc0404e97438f9c305a8df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.53      0.39      0.45       153\n",
      "     disgust       0.36      0.41      0.38        22\n",
      "        fear       0.20      0.20      0.20        40\n",
      "         joy       0.46      0.60      0.52       163\n",
      "     neutral       0.74      0.75      0.75       469\n",
      "     sadness       0.49      0.34      0.40       111\n",
      "    surprise       0.58      0.61      0.59       150\n",
      "\n",
      "    accuracy                           0.59      1108\n",
      "   macro avg       0.48      0.47      0.47      1108\n",
      "weighted avg       0.59      0.59      0.59      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43bb2d4b8bc4b5293880cfca74bdec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training Loss: 0.4416 | [1.111111111111111e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7715b9969a534fbc9f57bcaa9ffebeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.51      0.44      0.47       153\n",
      "     disgust       0.30      0.36      0.33        22\n",
      "        fear       0.19      0.15      0.17        40\n",
      "         joy       0.50      0.58      0.53       163\n",
      "     neutral       0.76      0.74      0.75       469\n",
      "     sadness       0.51      0.35      0.41       111\n",
      "    surprise       0.51      0.67      0.58       150\n",
      "\n",
      "    accuracy                           0.60      1108\n",
      "   macro avg       0.47      0.47      0.46      1108\n",
      "weighted avg       0.60      0.60      0.60      1108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d519d0904a654e62aee797129dd476a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training Loss: 0.3762 | [0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54d2157b17840ac8850e9b52d4d6df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.50      0.44      0.47       153\n",
      "     disgust       0.28      0.36      0.31        22\n",
      "        fear       0.20      0.15      0.17        40\n",
      "         joy       0.54      0.56      0.55       163\n",
      "     neutral       0.73      0.77      0.75       469\n",
      "     sadness       0.49      0.33      0.40       111\n",
      "    surprise       0.56      0.65      0.60       150\n",
      "\n",
      "    accuracy                           0.61      1108\n",
      "   macro avg       0.47      0.47      0.47      1108\n",
      "weighted avg       0.60      0.61      0.60      1108\n",
      "\n",
      "----- Final Test Performance -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34980b1f413d45d1816516a11af9a5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.48      0.43      0.46       345\n",
      "     disgust       0.23      0.22      0.23        68\n",
      "        fear       0.20      0.26      0.23        50\n",
      "         joy       0.58      0.55      0.56       402\n",
      "     neutral       0.78      0.78      0.78      1255\n",
      "     sadness       0.37      0.37      0.37       208\n",
      "    surprise       0.51      0.60      0.55       281\n",
      "\n",
      "    accuracy                           0.62      2609\n",
      "   macro avg       0.45      0.46      0.45      2609\n",
      "weighted avg       0.62      0.62      0.62      2609\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6203)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP Classifier\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from transformers import BertModel\n",
    "total_epochs = 10\n",
    "\n",
    "class WhisperClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, num_classes=7, train_whisper = False):\n",
    "        super(WhisperClassifier, self).__init__()\n",
    "\n",
    "        self.train_whisper = train_whisper\n",
    "        if train_whisper:\n",
    "            # self.audio_encoder = load_model(\"base\")\n",
    "            self.encoder = AudioEncoder(80, 1500, 512, 8, 6)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1500, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.bn_conv1 = nn.BatchNorm1d(128)\n",
    "        self.relu_conv1 = nn.ELU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4)\n",
    "\n",
    "        # Output from conv1d is [B, 128, input_dim // 2], flatten before FC\n",
    "        self.flatten_dim = (input_dim // 4) * 128\n",
    "        \n",
    "        self.compressor = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, hidden_dim*4),\n",
    "            nn.BatchNorm1d(hidden_dim*4),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # Text branch (BERT)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # self.text_fc = nn.Sequential(\n",
    "        #     nn.Linear(768, hidden_dim),\n",
    "        #     nn.BatchNorm1d(hidden_dim),\n",
    "        #     nn.ELU(),\n",
    "        #     nn.Dropout(0.2)\n",
    "        # )\n",
    "        \n",
    "        self.fc1 = nn.Linear((hidden_dim*4) + 768, hidden_dim*4)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim*4)\n",
    "        self.relu1 = nn.ELU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu2 = nn.ELU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim//4)\n",
    "        # self.bn3 = nn.BatchNorm1d(hidden_dim//4)\n",
    "        self.relu3 = nn.ELU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim//4, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2, x_mask):\n",
    "        # print(x.shape)\n",
    "        if self.train_whisper:\n",
    "            x1 = self.encoder(x1)\n",
    "        x1 = self.pool1(self.relu_conv1(self.bn_conv1(self.conv1(x1))))  #  [B, 128, input_dim // 2]\n",
    "        x1 = x1.view(x1.size(0), -1)  # flatten to [B, flatten_dim]\n",
    "        x1 = self.compressor(x1)\n",
    "        \n",
    "        # Text branch (BERT)\n",
    "        bert_outputs = self.bert(input_ids=x2, attention_mask=x_mask)\n",
    "        x2 = bert_outputs.last_hidden_state[:, 0, :]\n",
    "        # x2 = self.text_fc(x2)  \n",
    "        \n",
    "        # Concatenate audio + text\n",
    "        x = torch.cat([x1, x2], dim=1)  \n",
    "        \n",
    "        x = self.dropout1(self.relu1(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout2(self.relu2(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout3(self.relu3(self.fc3(x)))\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Instantiate model\n",
    "model = WhisperClassifier(input_dim=512, hidden_dim=256, num_classes=7, train_whisper=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(label_weights).to(device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=1e-5, \n",
    "                             weight_decay=1e-4\n",
    "                             )\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "model = load_model(model, \"base\")\n",
    "print(model)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "                                            num_warmup_steps=int(0.1 * total_epochs * len(train_loader)), \n",
    "                                            num_training_steps=int(total_epochs * len(train_loader)))\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, input_ids, attention_mask, y in tqdm(loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # X = normalize_x(X)\n",
    "            # X = transform_x(X)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            logits = model(X, input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "    wf1 = multiclass_f1_score(torch.from_numpy(np.array(all_preds, dtype=np.int64)), \n",
    "                              torch.from_numpy(np.array(all_labels, dtype=np.int64)), \n",
    "                              num_classes=7, average=\"weighted\")\n",
    "    return wf1\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, dev_loader, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            X, IIDS, AM, y = batch\n",
    "            X, IIDS, AM, y = X.to(device), IIDS.to(device), AM.to(device), y.to(device)\n",
    "            # X = normalize_x(X)\n",
    "            # X = transform_x(X)\n",
    "            y = y.type(torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X, IIDS, AM)\n",
    "            \n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            # break\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {total_loss/len(train_loader):.4f} | {scheduler.get_last_lr()}\")\n",
    "        acc = evaluate_model(model, dev_loader)\n",
    "        acc = str(acc).replace('.', '_')\n",
    "        torch.save(model.state_dict(), f\"./checkpoints/epoch_{epoch}-acc_{acc}.pth\")\n",
    "        # break\n",
    "\n",
    "train_model(model, train_loader, dev_loader, epochs=total_epochs)\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"----- Final Test Performance -----\")\n",
    "evaluate_model(model, test_loader) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "151fd2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semal\\AppData\\Local\\Temp\\ipykernel_33624\\991987500.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  avg_wts = torch.load(pth_paths[0], map_location='cpu')\n",
      "C:\\Users\\semal\\AppData\\Local\\Temp\\ipykernel_33624\\991987500.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wts = torch.load(path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1f6bd0a08b46a5bd8256431c096ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.56      0.42      0.48       153\n",
      "     disgust       0.35      0.36      0.36        22\n",
      "        fear       0.22      0.12      0.16        40\n",
      "         joy       0.53      0.58      0.55       163\n",
      "     neutral       0.77      0.72      0.74       469\n",
      "     sadness       0.40      0.51      0.45       111\n",
      "    surprise       0.56      0.69      0.62       150\n",
      "\n",
      "    accuracy                           0.61      1108\n",
      "   macro avg       0.48      0.49      0.48      1108\n",
      "weighted avg       0.61      0.61      0.60      1108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "def average_weights(pth_paths: List[str]) -> dict:\n",
    "    assert len(pth_paths) > 0, \"No checkpoint paths provided.\"\n",
    "\n",
    "    # Load first model weights\n",
    "    avg_wts = torch.load(pth_paths[0], map_location='cpu')\n",
    "\n",
    "    # Initialize accumulator\n",
    "    for key in avg_wts.keys():\n",
    "        avg_wts[key] = avg_wts[key].clone()\n",
    "\n",
    "    # Accumulate other weights\n",
    "    for path in pth_paths[1:]:\n",
    "        assert isfile(path), f\"Checkpoint file {path} does not exist.\"\n",
    "        wts = torch.load(path, map_location='cpu')\n",
    "        for key in avg_wts.keys():\n",
    "            avg_wts[key] += wts[key]\n",
    "\n",
    "    # Average\n",
    "    for key in avg_wts.keys():\n",
    "        if avg_wts[key].dtype==torch.int64:\n",
    "            print(avg_wts[key].dtype)\n",
    "            avg_wts[key] = avg_wts[key]//len(pth_paths)\n",
    "        else:\n",
    "            avg_wts[key] /= float(len(pth_paths))\n",
    "\n",
    "    return avg_wts\n",
    "\n",
    "state_dict = average_weights([\n",
    "    # join(os.getcwd(), \"checkpoints/epoch_9-acc_tensor(0_3552).pth\"),epoch_5-acc_tensor(0_3910).pth\n",
    "    join(os.getcwd(), \"checkpoints/epoch_6-acc_tensor(0_6013).pth\"),\n",
    "    join(os.getcwd(), \"checkpoints/epoch_9-acc_tensor(0_5987).pth\"),\n",
    "    join(os.getcwd(), \"checkpoints/epoch_1-acc_tensor(0_5908).pth\"),\n",
    "    ])\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "acc = evaluate_model(model, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39624cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
